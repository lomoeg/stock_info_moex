# -*- coding: utf-8 -*-
"""task3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rZAd0iciWbN1XQ-T8QuK1cqkc7pc_08O
"""

#imports, data loading, concat datasets, select shop
import pandas as pd
import numpy as np
import xgboost as xgb
from datetime import timedelta
import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error

from google.colab import drive
drive.mount('/content/drive')

from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv("stock_info.csv")
data

data.drop(columns=['moving average', 'last_price'], inplace=True)
data.dropna(inplace=True)
data

nomenclatureId = {}
idNomenclature = {}
k = 1

for idx,row in data.iterrows():
  if row['short_ticker'] not in nomenclatureId:
    nomenclatureId[row['short_ticker']] = k
    idNomenclature[k] = row['short_ticker']
    k += 1

nomenclatureId

date_simulate = '2020-04-03'
dataset = data.copy()
dataset.reset_index(inplace=True)
dataset.drop(columns=['index'], inplace=True)
dataset['Date'] = dataset['trade_date']
dataset['Date'] = pd.to_datetime(dataset['Date'], format="%Y-%m-%d")
dataset.drop(columns=['trade_date'], inplace=True)
dataset.sort_values(by=['Date'], inplace=True)
fake = dataset.copy()

def simulation(dataset):
    a = dataset.loc[dataset.shape[0]-1, 'Date']
    gg = dataset[dataset['Date'] == a]
    gg.reset_index(inplace=True)
    a = a + timedelta(days=1)
    gg['Date'] = gg['Date'].apply(lambda x: a)
    gg.drop(columns=['index'], inplace=True)
    dataset = dataset.append(gg)
    dataset.reset_index(inplace=True)
    dataset.drop(columns=['index'], inplace=True)
    return dataset, a

def predict(dataset):
  #create simulation
  #make a simulsation addition to dataset

  dataset, date_simulate = simulation(dataset)

  #previous n weeks
  for i in range(1,6):
    column_name_1 = 'PreviousWeek' + str(i)
    column_name_2 = 'PreviousWeek' + str(i) + 'close_price'
    dataset[column_name_1] = dataset['Date'].apply(lambda x: x - timedelta(days=7*i))
    faker = fake.copy()
    faker[column_name_1] = faker['Date']
    faker[column_name_2] = faker['close_price']
    faker = faker.drop(columns=['Date', 'close_price'])
    dataset = pd.merge(dataset, faker, how='left', on=[column_name_1, 'short_ticker'])
  #previous n days
  for i in range(1,4):
    column_name_1 = 'PreviousDay' + str(i)
    column_name_2 = 'PreviousDay' + str(i) + 'close_price'
    dataset[column_name_1] = dataset['Date'].apply(lambda x: x - timedelta(days=i))
    faker = fake.copy()
    faker[column_name_1] = faker['Date']
    faker[column_name_2] = faker['close_price']
    faker = faker.drop(columns=['Date', 'close_price'])
    dataset = pd.merge(dataset, faker, how='left', on=[column_name_1, 'short_ticker'])

  drop_until = datetime.date(2017, 5, 10)
  dataset['Previous_1_exist'] = dataset['Date'].apply(lambda x : (x >= drop_until))
  dataset = dataset[dataset['Previous_1_exist'] == True]
  dataset.fillna(0, inplace=True)
  dataset = dataset.reset_index()
  #drop remember
  dataset.drop(columns=['index', 'Previous_1_exist', 'PreviousWeek1', 'PreviousWeek2', 'PreviousWeek3', 'PreviousWeek4', 'PreviousWeek5'], inplace=True)
  dataset.drop(columns=['PreviousDay1', 'PreviousDay2', 'PreviousDay3'], inplace=True)

  def cols_new_date(data_df):
    data_df['year'] = data_df['Date'].dt.year - 2017
    data_df['quarter'] = data_df['Date'].dt.quarter
    data_df['month'] = data_df['Date'].dt.month
    data_df['weekofyear'] = data_df['Date'].dt.weekofyear
    data_df['dayofweek'] = data_df['Date'].dt.dayofweek
    data_df['dayofyear'] = data_df['Date'].dt.dayofyear


    return data_df

  dataset = cols_new_date(dataset)
  dataset.sort_values(by=['Date'], inplace=True)
  dataset['short_ticker'] = dataset['short_ticker'].apply(lambda x: nomenclatureId[x])
  train = dataset[dataset['Date'] < pd.to_datetime(date_simulate)]
  test = dataset[dataset['Date'] >= pd.to_datetime(date_simulate)]


  return dataset, train, test

dataset, train, test = predict(dataset)
dataset

def predict_model(dataset, train, test):
  X = train.drop(columns=['close_price', 'Date'])
  y = train['close_price']


  X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.01)
  print(X_train.shape + X_test.shape)

  X_test_t= test.drop(columns=['close_price', 'Date'])
  y_test_t = test['close_price']

  model = xgb.XGBRegressor(
      max_depth=10,
      n_estimators=1000,
      min_child_weight=300, 
      colsample_bytree=0.8, 
      subsample=0.8, 
      eta=0.3,    
      seed=42)

  model.fit(
      X_train, 
      y_train, 
      eval_metric="mae", 
      eval_set=[(X_train, y_train), (X_test, y_test)], 
      verbose=True, 
      early_stopping_rounds = 10)
  
    #prediction
  predict = model.predict(X_test_t)
  predict = pd.DataFrame({'Model': predict})
  predict = np.round(predict)

  print("MSE:")
  print(mean_squared_error(y_test_t, predict))
  print("MAE:")
  print(mean_absolute_error(y_test_t, predict)) 

  datasetTest = test.copy()
  datasetTest.reset_index(inplace=True)
  return datasetTest[['short_ticker',	'close_price', 'Date']]

gg = predict_model(dataset, train, test)
gg

def connect(dataset, gg):
  gg['short_ticker'] = gg['short_ticker'].apply(lambda x: idNomenclature[x])
  dataset = dataset.append(gg)
  dataset.reset_index(inplace=True)
  dataset.drop(columns=['index'], inplace=True)
  return dataset



m = connect(fake, gg)
m

###here scripts start##########
dataset = data.copy()
dataset['Date'] = dataset['trade_date']
dataset['Date'] = pd.to_datetime(dataset['Date'], format="%Y-%m-%d")
dataset.drop(columns=['trade_date'], inplace=True)
dataset.sort_values(by=['Date'], inplace=True)
dataset.reset_index(inplace=True)
dataset.drop(columns=['index'], inplace=True)

#input num of days
for i in range(90):
  fake = dataset.copy()
  dataset, train, test = predict(dataset)
  gg = predict_model(dataset, train, test)
  m = connect(fake, gg)
  dataset = m.copy()

dataset

dataset.to_csv("predict_company.csv")
